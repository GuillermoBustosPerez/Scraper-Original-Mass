---
title: 'Scraper original mass: a different approach and its wide range implementation'
author: "Guillermo Bustos-Pérez"
output:
  md_document:
    variant: gfm
bibliography: "References.bib"
csl: "plos-one.csl"
link-citations: true
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Scraper original mass: a different approach and its wide range implementation**   

Guillermo Bustos-Pérez $^{(1,2,3)}$

$^{(1)}$ Department of Human Origins, Max Planck Institute for Evolutionary Anthropology, Leipzig.  
$^{(2)}$ Institut Català de Paleoecologia Humana i Evolució Social (IPHES-CERCA), Tarragona, Spain.  
$^{(3)}$ Departament d’Història i Història de l’Art, Universitat Rovira i Virgili, Tarragona, Spain.  


<div align="justify">   

## **Abstract**   

Predicting the original mass of a retouched scraper has long been a major goal in lithic analysis. It is commonly linked to lithic technological organization of past societies along with notions of stone tool general morphology, standardization through the reduction process, use life, and site occupation patterns. In order to obtain a prediction of original stone tool mass, previous studies have focused on attributes that would remain constant or unaltered through retouch episodes. However, these approaches have provided limited success for predictions and have also remained untested in the framework of successive resharpening episodes. In the research presented here, a set of experimentally knapped flint flakes were successively resharpened as scraper types. After each resharpening episode, four attributes were recorded (scraper mass, height of retouch, maximum thickness and the GIUR index). Four machine learning models were trained using these variables in order to estimate the mass of the flake prior to any retouch. A Random Forest model provided the best results with an r2 value of 0.97 when predicting original flake mass, and a r2 value of 0.84 when predicting percentage of mass lost by retouch.  The Random Forest model has been integrated into an open source and free to use Shiny app. This allows for the wide spread implementation of a highly precise machine learning model for predicting initial mass of flake blanks successively retouched into scrapers.    

**Key words:** experimental archaeology; lithic reduction; flake mass; machine learning.   

## **Introduction**   

Scrapers are some of the most common lithic implements among archaeological lithic assemblages. They are present from the first Oldowan stone tools [@barsky_early_2011; @sahnouni_further_2002; @semaw_26-million-year-old_2003] through to modern ethnographic studies of hunter gatherers [@casamiquela_temas_1978; @gallagher_contemporary_1977; @shott_measuring_2007; @sillitoe_living_2003]. The "reduction model" [@dibble_middle_1995; @dibble_interpretation_1987] suggests that some stone tools (including scrapers) can represent different stages of reuse and modification through retouch. When considering scrapers within the reduction model, an integral concept is that of curation. The initial definition of curation included a series of behavioral patterns related to provisioning strategies [@binford_organization_1979; @renfrew_interassemblage_1973]. However, further authors included into curation behavioral strategies such as tool transport, utilization in a wide range of tasks, anticipated production, hafting, and recycling (after the original tool had been discarded). An alternative definition of curation was proposed by Shott [@shott_exegesis_1996; @shott_tool-class_1989], who defined curation as “the ratio of realized to potential utility”. This new approach to the definition of curation has important implications for lithic analysis and the study of lithic technological organization since it transforms curation into a continuous variable [@shott_exegesis_1996]. Given this new definition, and in the framework of the reduction model, the amount of mass lost by a lithic artifact by reuse/resharpening will be equivalent to its ratio of curation. Both variables (amount in grams and percentage of mass lost by a stone tool) can be empirically calculated in experimental contexts and estimated in archaeological contexts. The presence of scrapers and their curation relates to several aspects of the organization of lithic technology of past societies [@clarkson_holocene_2002; @glauberman_late_2020; @andrefsky_construction_2008; @kuhn_unpacking_1991; @shott_costs_2018; @shott_use_2017]. Amount of curation affects the shape of stone tools at the moment of their discard (thus affecting the morphological variability of stone tool assemblages observed through time). Amount of curation can also relate to raw material sources, with more curated artifacts coming from longer distances. Curation also relates to the selection of technological products for more intensive retouch, or shifts in technological strategies of transport, thus informing about the cultural choices and patterns of past human groups. Finally, curation also relates to tool use and use-wear analysis.   

Because of these reasons, predicting original scraper mass is a major goal in lithic analysis. Thus far, two approaches are employed to estimate the reduction and curation undergone by a retouched artifact. The first approach uses estimations derived from direct measurements on retouch. This has led to the proposal of several indexes using different measurements, such as height of retouch, length of retouched edge or projection of the original angle [@bustos-perez_exploring_2019; @eren_defining_2005; @kuhn_geometric_1990; @morales_measuring_2015]. These indices usually provide good correlation values with mass lost, but they are usually conditioned by flake morphology, direction of retouch, or tool type (laterally retouched scrapers, endscrapers, bifacial products, etc.) and each one uses different scale ranges.    

The second approach aims at estimating original flake mass based on remaining flake features. This approach has the advantage of not being limited by tool type, direction of retouch or flake morphology. Initial work focused on controlled experiments of flake formation using different measures of flake platform (width, depth) and exterior platform angle (EPA) to estimate flake mass [@dibble_platform_1997; @dibble_effect_1995; @dibble_new_1981; @pelcin_formation_1997; @pelcin_controlled_1996]. Some of the reasons to select these features were that they usually remain unaltered in most retouched artefacts. These controlled experiments provided strong explanatory power for the formation of flakes, with flake mass being predicted with an r2 value above 0.8 [@dibble_effect_1995; @li_synthesis_2023]. However, when the same variables are used to predict mass of experimentally knapped flakes, the predictive power of the model diminishes significantly, with r2 values dropping to 0.403 [@davis_quantifying_1998] (0.224 for the same retouched flakes), and 0.384 [@shott_flake_2000]. These results meant an important drawback since, as Dibble [@dibble_comment_1998 :611] states: “controlled experiments, in spite of their elegance, objectivity and replicability, are only useful if the results obtained from them are directly applicable to archaeological materials”.    

To overcome the limitations from these results, three approaches are commonly undertaken:   

  a) Adding additional features as predictive variables. Commonly flake thickness is added [@shott_use_2017; @dogandzic_edge_2015], since it is widely considered to remain unaltered through the reduction process. Other variables, such as scar count or remaining amount of cortex, seem to improve the predictive power of models [@bustos-perez_predicting_2021; @bustos-perez_multiple_2022].   
  b) Applying new methods for measuring more accurately existing variables. Examples are the refinement on traditional manual measurements of platform [@muller_new_2016], the use of digital photographs [@braun_landscape-scale_2008], or 3D scans for measuring platform [@clarkson_estimating_2011; @maloney_experimental_2020].   
  c) Applying different families of transformations in order to favor the Gaussian distribution of values of predictors and flake mass, thus increasing the predictive power of most models. These transformations usually use the cubic root [@li_synthesis_2023; @dogandzic_edge_2015; @dogandzic_results_2020] or different logarithmic transformations [@davis_quantifying_1998; @shott_flake_2000; @bustos-perez_predicting_2021; @bustos-perez_multiple_2022; @clarkson_estimating_2011; @maloney_experimental_2020].    
  
It can be considered that these additions and improvements have provided correlation values of original flake mass on scrapers which allow for comparisons at the assemblage level. However, estimations at the individual stone artifact level remain unsatisfactory with a limited application to archaeological cases.  This is due to three main reasons. First, while most research explores extensively the prediction of mass through different variables (and their interactions), it is usually not considered in the frame of continuous resharpening process, and when it is tested in this framework (continuous or single episodes of retouch), results provide lower correlation values [@dibble_comment_1998; @maloney_experimental_2020].  Second, while most research aims at estimating original flake mass, less research provides results of estimations of percentage of mass lost against actual percentage of mass lost during retouch, which is the key component of the curated concept [@shott_measuring_2007; @shott_exegesis_1996]. Third, most archaeological research addressing the prediction of original flake/scraper mass result in equations which might be difficult to extrapolate and practically apply other archaeological assemblages [@shott_use_2017; @davis_quantifying_1998; @morales_distribution_2016]. Recently [@bustos-perez_predicting_2021; @bustos-perez_multiple_2022], the use of machine learning has allowed the implementation of feature selection (identification of how many and which variables are better for prediction) and new algorithms. However, it has also resulted in limited improvements of the correlation coefficient [@bustos-perez_multiple_2022], indicating that a possible threshold limit for this approach is being reached.    

A new framework is needed to overcome the limitations of previous models (absence of being tested in sequential experimentations, higher accuracy, and easy implementation for all lithic analysts) aimed at predicting original flake mass from scraper attributes and being able to reach the individual scraper level. In the present study 134 flakes were successively retouched, providing a dataset of 694 episodes of resharpening. After each retouch episode, a series of attributes were measured and used to train four machine learning models. A Random Forest model provided the highest r2 value (0.974) when estimating scraper original mass, and the highest r2 (0.839) when estimating percentage of mass lost by retouch. The Random Forest model and all training data are implemented through a Shiny app “Original Scraper Mass Calculator v.1.0”, which allows the user to manually introduce the data from a scraper to estimate its original mass or to upload all data at once and download the results.    

## **Materials and methods.**   

### **Experimental sample.**  
  
The analyzed sample consisted of 134 experimentally knapped flakes using hard hammer. The raw material of hammerstones varied widely (quartz, quartzite, sandstone, and limestone), which allowed for a diverse range of morphologies and potential active percussion areas. The experimental sample is dominated by flakes with feather terminations (n = 121; 90.3%), followed by flakes with hinge terminations (n = 10; 7.46%).    

```{r load-data-libraries, warning=FALSE, message=FALSE}
Data1 <- read.csv("Data/Data1.csv", sep = " ")
load("Data/Data2.RData")
library(tidyverse); library(caret)
```


Initial flake mass was recorded using a Sytech SY-BS502 with a precision of 0.01 grams. Average weight of the samples was 47.38 g, with 50% of flakes weighing between 18.07 and 63.16 g, and a standard deviation of 36.48. Figure 1 presents the flake mass distribution for the experimental sample, indicating a long tail of 14 flakes weighing more than 100 g.   

The transversal section of flakes is considered to have an important effect on estimations derived from the geometric index of unifacial reduction (GIUR) [@kuhn_geometric_1990] and height of retouch. In particular, when a flake’s dorsal surface is parallel to the ventral surface, the GIUR and height of retouch will only marginally or will not at all increase after each resharpening episode, resulting in underestimations of flake mass removal [@dibble_middle_1995; @eren_kuhns_2009]. However, the actual effect of the “flat flake problem” on the estimation of flake mass might be marginal [@hiscock_reality_2009; @hiscock_experimental_2005]. The present study recorded flake schematic transversal section prior to retouch of each flake, with possible categories being: circular (n = 20), triangular (n = 63), triangular asymmetric (n = 29), trapezoidal (n = 13) and trapezoidal asymmetric (n = 9). The first three categories are considered to represent cases where the “flat flake problem” is not present, while the latter two are consider to represent cases were this problem is present.

```{r}
Data0 <- read.csv("Data/Data0.csv", sep = ",")

Summary_Assem <- data.frame(
  rbind(data.frame(data.matrix(summary(Data0$L.mm))) %>% t(),
        data.frame(data.matrix(summary(Data0$W.mm))) %>% t(),
        data.frame(data.matrix(summary(Data0$T.mm))) %>% t(),
        data.frame(data.matrix(summary(Data0$Max.Thick.mm))) %>% t(),
        data.frame(data.matrix(summary(Data0$Mean.Edge.Angle.Selected.ret))) %>% t()))
Measure <- c("Length", "Width", "Middle Thickness", "Maximum thick",
             "Mean angle edge prior to retouch")
Summary_Assem <- cbind(Measure, Summary_Assem)
rownames(Summary_Assem) <- 1:nrow(Summary_Assem)
print(Summary_Assem)
rm(Data0)
```

```{r histogram-initial-mass}
Data1 %>% 
  filter(Episode == 0) %>% 
  ggplot(aes(Or.Weight.g)) +
  geom_histogram(binwidth = 10,
                 color = "black", fill = "gray", 
                 boundary = 0) +
  theme_light() +
  ylab("Frequency") +
  xlab("Mass (g)") +
  scale_x_continuous(breaks = seq(0, 210, 10)) +
  scale_y_continuous(breaks = seq(0, 30, 2)) +
  theme(
    axis.text = element_text(color = "black", size = 6),
    axis.title = element_text(color = "black", size = 10, face = "bold"))
```
\ 

All flakes were retouched until they were too small to hold while retouching (n = 4), they broke during retouch (n = 74), or the angle of retouch was too abrupt to detach additional resharpening flakes (n = 59). Retouch was done through freehand direct hard hammer on the dorsal face of flakes (direct retouch). In order to reshape the flakes one continuous retouched edge was established. After the first episode of resharpening, the retouched edge was expanded through the flakes edge in a continuous manner. This limits the potential application of this index to simple scrapers with direct retouch (which excludes double scrapers or scrapers with inverse or bifacial retouch).   

Most flakes underwent between four and seven episodes of retouch (67.2%), while only seven flakes provided nine or more episodes of retouch. The experimental assemblage provided a total of 694 resharpening episodes with which to train the predictive models.   

### **Feature selection.**    

Based on previous research [@kuhn_geometric_1990; @dogandzic_edge_2015; @bustos-perez_predicting_2021; @hiscock_experimental_2005] variables were recorded as predictive features. After each episode of retouch, the following variables were recorded:    

  * Remaining scraper mass, recorded in grams with a Sytech SY-BS502 scale and a precision of 0.01 g. This variable is selected since machine learning models will consider remaining flake mass as a baseline on minimum mass of the scraper.   
  * Thickness at the midpoint of the flake (measured in mm with a precision of 0.1). It is considered that thickness remains relatively unchanged as resharpening increases [@shott_use_2017], and its addition tends to increase the predictive power of models [@dogandzic_edge_2015].   
  * Maximum thickness of the flake measured in mm (with a precision of 0.1). Feature selection through all possible combination of variables indicates that the logarithmic (base 10) transformation of this variable can increase the predictive power of regression models for freehand knapped flakes. Logarithmic transformation can result in Gaussian distribution of feature values increasing the predictive power of a model [@bustos-perez_predicting_2021]. It is also considered that as resharpening proceeds, the thickness at the midpoint will be displaced (since length and width will diminish), while maximum thickness will remain more stable.   
  * Three equidistant measures of height of retouch (*t*) and the corresponding thickness (*T*) of the flake [@hiscock_experimental_2005] measured in mm (with a precision of 0.1). The average of these three points is used as a predictive feature. Here it is considered that the average height of the retouch will serve as a proxy of mass removed from the scrapper.    
  * The GIUR index proposed by Kuhn [@kuhn_geometric_1990]. This index divides the height of retouch (*t*) by its corresponding thickness (*T*). As previously indicated, the present study records three equidistant heights of retouch (*t*), each being divided by their corresponding flake thickness (*T*). The GIUR value is calculated as the average of these three divisions. GIUR values can range from 0 (unretouched flake) to 1 (when the height of the reaches the dorsal side of a flake).    
Variables selected for training the regression models were: scraper mass, maximum thickness (log transformed), average height of retouch (t) and value of the GIUR index.   

### **Regression models and evaluation.**    

Four methods were employed for regression analysis: multiple linear regression, support vector regression with a linear kernel, random forest and gradient boosting machine.    

Multiple linear regression (MLR) extends the simple linear regression in such a way that it can directly accommodate multiple predictors [@james_introduction_2013 : 71].      

Support vector machines for regression [@awad_support_2015; @cortes_support-vector_1995; @smola_tutorial_2004] with a linear kernel (SVML) fit a linear hyperplane and a margin of error which allows for errors of points falling inside the margin. Points falling outside the margin define the support vectors. This provides a model focused on the general trend which aims to maximize the margin while minimizing the error, and which is also robust to the presence of outliers.      

Random forest for regression selects random samples of the data and builds decision trees for prediction [@breiman_random_2001]. As a result, each tree is built from different combinations of the data, and the average is used as prediction. This adds diversity, reduces overfit, and provides high-accuracy predictions [@lantz_machine_2019].    

The gradient boosting machine (GBM) is an ensemble method that builds up a final model by incrementally improving an existing one [@friedman_greedy_2001; @friedman_stochastic_2002]. The first model uses an initial *“shallow tree”* with a constant value (average of the labels). Following this initial model, a new tree (*weak learner*) is fitted to predict the residuals of the model, contributing to the final model predictions, allowing for correction of the errors of the model. This process is repeated, allowing it to progressively identify the shortcomings of week learners on a sequence of decision trees and to reduce the errors of the ensemble predictions.    

Initial models are trained to estimate original scraper mass based on the set of selected attributes. However, as previously stated, the objective is to evaluate the ability of a model to predict the curation ratio of a stone tool (percentage of mass remaining relative to its original mass). Calculating the curation ratio of a stone tool can be formulated as:    

$$ 100 - ((M/EOM) * 100) $$   

Where:    
M = mass (directly measured on the scraper).   
EOM = estimated original mass (provided by the model).   

Models of both predictions (original scraper mass and curation ratio) are compared using four measures of performance: $r^2$, MAE, RMSE, and MAPE.  $r^2$ is a measure of linear correlation and of how much of the observed variation is explained by the model [@james_introduction_2013]. In lithic studies, a categorization of the predictive power of indices has been proposed based on their $r^2$ values, where <0.1 is low, 0.1–0.25 is moderate, 0.26–0.5 is fairly large/strong, 0.51–0.8 is very large/strong and >0.8 is extremely large/strong. However, it is important to consider that different distributions of data can result in same or similar $r^2$ values [@anscombe_graphs_1973].   

Mean average error (MAE), root mean squared error (RMSE), and mean average percentual error (MAPE) provide summary values of how far predictions fall from the true value [@james_introduction_2013; @lantz_machine_2019]. MAE measures the average magnitude of errors, regardless of signal. RMSE also provides a measure of distance between predicted and actual values, although it punishes large errors. RMSE is usually compared to the standard deviation (SD) of the variable to be predicted. If RMSE presents a lower value than the SD, this is indicative of a good model which predicts values better than taking the average value of the sample. MAPE provides a measure of distance on a proportional basis (a residual of 3 g in a 12 g flake will be much higher than in a 50 g flake). A perfect model has a MAE, RMSE, and MAPE values of 0, and in general, better models will have lower values of MAE, RMSE, and MAPE.      

Collinearity of the predictors is addressed through the variance inflation factor (VIF). VIF provides a measure of correlation between predictors and their effects on the model. In the present study VIF is calculated using the R package car v.3.1.2. [@fox_r_2018]. Common thresholds for VIF values [@marquardt_generalized_1970; @obrien_caution_2007] range between 1 to 10 (considered inconsequential); 10 to 30 (cause for concern), and higher than 30 (seriously harmful). At present, the package car v.3.1.2 only allows for the calculation of the variance inflation factor for multiple linear regression. Although the different nature of the regression algorithms can result in different effects of collinearity, results from calculating the variance inflation factor in the multiple linear regression can be extrapolated to the rest of the models.    

Models were evaluated using a k-fold cross validation. In k-fold cross validation, the dataset is randomly shuffled and divided into k folds. The first fold is employed as a test set, and the model is trained in the remaining folds. After this, the second fold is employed as a test set and the rest as a new training set. This process continues until all folds have served as a test set. Since the samples in each fold are determined by the initial random shuffle, it is advisable to repeat this cycle a series of times. The present work employs a 10-fold cross validation (six folds having a sample of 69 elements, and four folds having a sample of 70 elements) which is repeated 50 times with an initial random shuffling.    

In addition to the above listed performance metrics, all models are graphically evaluated. A regression plot provides a scatter plot of predicted and true values along its regression line. In a good model, the regression line will pass through the center of all points, which will be evenly distributed above and below, and outliers are visible.    

The distribution of residuals (difference between predicted and actual values) allows for an evaluation of bias in the model. In the present study residuals are used in combination with transversal section and different intervals of the GIUR index in order to determine possible bias or limitations of the model. Residuals plots of a good model will have the points evenly distributed around zero across the range of the independent variables, and no statistical difference will exist between the categories of flake transversal section or GIUR intervals. Evaluation of residuals is only undertaken for the considered best model.     

The evaluation of models using the k-fold cross validation was performed using the complete dataset of 698 resharpening episodes from the 134 flakes. This implies that, for a prediction, previous and posterior resharpening episodes of the same flake were included in the training data set. This raises the question whether the model is overfitting from seeing previous and posterior resharpening examples of the same flake. In order to evaluate this possible source of overfitting, a random selection of 10% (n = 13) of flakes and all their resharpening episodes were removed from the training set and the remaining sample used to train the previously selected best model. This process was repeated until 100000 predictions were obtained and then the four measures of performance were calculated. In this scenario, a significant decrease in model performance suggests overfitting in the full sample that includes multiple stages of the same flake.   

Additionally, two models were trained for comparison using the sample of 134 flakes. The first model uses the same set of variables from Bustos-Pérez & Baena Preysler [@bustos-perez_multiple_2022] in order to predict original flake mass (log transformed). The second model was used log transformations of platform size and maximum thickness along with exterior platform angle as predictive variables in order to predict the cubic root of flake mass. Both models were trained using a multiple linear regression, and predicted values were transformed to the linear scale in order to compare performance metrics with those of the best model from the current study.   

The complete workflow was developed in RStudio IDE v.2024.04.02 [@rstudio_team_rstudio_2019] using the R programming language v.4.4.1 [@r_core_team_r_2019]. The package tidyverse v.2.0.0 [@wickham_welcome_2019] was employed for data manipulation and representation. Multiple linear regression uses the R package MASS v.7.3.60.2 [@venables_modern_2002], SVM with linear kernel uses package kernlab v.0.9.32 [@karatzoglou_kernlab_2004], the random forest model uses packages e1071 v.1.7.14 and ranger v.0.16.0 [@dimitriadou_misc_2008; @wright_ranger_2015], and GBM uses package gbm v.2.2.2 [@ridgeway_generalized_2007]. Training and validation of models was done using the package caret v.6.0.94 [@kuhn_building_2008]. The “Original Scraper Mass Calculator” which implements the model through a user friendly interface was written using the shiny package v.1.8.1.1 [@chang_package_2015; @kasprzak_six_2020; @wickham_mastering_2021]. All data, code is made publicly available through a public repository organized following the structure of a research compendium [69] and using a markdown document through package bookdown v.0.39 [@mcnamara_dynamic_2014; @xie_bookdown_2016; @xie_knitr_2014].   
All data and the complete workflow of analysis is available as a research compendium [@marwick_packaging_2018] at Github (https://github.com/GuillermoBustosPerez/Scraper-Original-Mass/tree/main). The complete code and files of the Original Scraper Mass calculator v.1.0.0 is also available at Github (https://github.com/GuillermoBustosPerez/Original-Scraper-Mass-Calculator), and the final implementation of the application can be accessed at: https://guillermo-bustos-perez.shinyapps.io/Original-Scraper-Mass-Calculator/    

### Training of the models   

The following script trains the above described models. For the SVM linear regression, random forest and GBM hyperparameter cartesian grid search is performed, and final models are trained with the best selection of hyperparameters. Final models can be accessed at [Models](Models) folder.   

```{r, eval=FALSE}
source("/Scripts/11-Training-models.R")
```

The following line of code trains the random forest model by removing all resharpening events of 10% of flakes. This is dne in order to determine if the model is benefiting from seeing previous episodes of a same flake, thus resulting in overfit.   

```{r, eval=FALSE}

```



```{r}
load("Models/Best-Subset-Random-Forest.RData")
load("Models/Best-Subset-MLR.RData")
load("Models/Best-Subset-SVM-Linear.RData")
load("Models/Best-Subset-GBM.RData")
```


## **Results**      

### **Resharpening effects on the experimental assemblage**   

Figure 2 presents the effects of each resharpening episode on the experimental assemblage. On average, flakes from the first episode of retouch had 1.84 g removed (with the exception of an outlier flake which had 25 g removed). Maximum value of mass removed was of 95.5 g after five episodes of retouch, while flakes reaching ten episodes of retouch had an average of 28.4 g removed. As for the retouched pieces, mass of the assemblage decreased from 45.9 g after the first episode of retouch to 15.9 g after ten episodes of retouch. On average, the resharpening episodes removed 20% (with a standard deviation of 15.9%) of mass from the scrapers. One resharpening episode removed a minimum of 0.513%, and a resharpening episode removed a maximum of 67.3% of mass. 50% of the resharpening episodes removed between 5.9% and 31.3% of mass.   

```{r effects-resharpening}
# Resharpenning 
Data2 %>% select(Episode, GIUR, W.Retrieved.g, Mean.t, Rem.Weight.g) %>% 
  pivot_longer(
    cols = c(GIUR:Rem.Weight.g),
    values_to = "Values",
    names_to = "Variables") %>% 
  
  ggplot(aes(as.factor(Episode), Values)) +
  facet_wrap(~ factor(Variables, 
                      levels = c("W.Retrieved.g", "Rem.Weight.g", "GIUR",  "Mean.t"),
                      labels = c( "Mass Removed (g)", "Remaining mass (g)", "GIUR", "Mean t (mm)")) , 
             scales = "free") +
  geom_violin() +
  geom_boxplot(width = 0.4) + 
  theme_light() +
  xlab("Resharpening event") +
  theme(
    axis.text = element_text(color = "black", size = 7),
    axis.title = element_text(color = "black", size = 7),
    strip.text = element_text(color = "black", face = "bold", size = 8),
    strip.background = element_rect(fill = "white", colour = "black", linewidth = 1))
```
\  

In general, the GIUR remains fairly unidirectional, with increasing values after each episode of resharpening. The average GIUR after the first resharpening episode was 0.33. The first maximum GIUR value (1) was reached after six episodes of resharpening. Only six flakes of the experimental assemblage reached a GIUR values of 1, providing a total of eight different episodes. Mean height of retouch (t) increases during the first seven resharpening episodes, from an average value of 4.19 mm after the first episode, to a value of 11.4 mm after the seventh episodes. Average height of retouch (t) decreases progressively afterwards to a value of 10.8 mm on episode 10 of retouch. This decrease in average t is result of retouch reaching the upper dorsal side of flakes and diminishing the overall thickness.     
Both thickness at the midpoint and maximum thickness (Figure 3) remain fairly constant among all the resharpening events, although the range of values decreases with each resharpening episode (this is due to a decrease in the number of flakes due to discard). After eight resharpening events, a light decrease in both measures of thickness is observed due to retouch reaching the upper dorsal part of the flake and reducing its overall thickness.   

```{r thickness-resharpening}
Data2 %>% select(Episode, T.mm, Max.Thick.mm) %>% 
  pivot_longer(
    cols = c(T.mm:Max.Thick.mm),
    values_to = "Values",
    names_to = "Variables") %>% 
  
  ggplot(aes(as.factor(Episode), Values)) +
  facet_wrap(~ factor(Variables, 
                      levels = c("T.mm", "Max.Thick.mm"),
                      labels = c("Thickness (mm)", "Max. Thick. (mm)")) , 
             scales = "free") +
  geom_violin() +
  geom_boxplot(width = 0.4) + 
  theme_light() +
  xlab("Resharpening event") +
  theme(
    axis.text = element_text(color = "black", size = 7),
    axis.title = element_text(color = "black", size = 7),
    strip.text = element_text(color = "black", face = "bold", size = 8),
    strip.background = element_rect(fill = "white", colour = "black", linewidth = 1))
```
\   

### **Evaluation of regression models**     

Table 2 presents performance metrics of each of the regression models for predicting original flake mass. All models had RMSE values lower than the standard deviation of original flake mass, indicating a good fit of the models. The random forest model presented the best values for all evaluation metrics, being able to capture 0.974 of variance, and with a MAE of 3.297 g. It also presented the lowest RMSE value (5.917), indicative of being the least affected by outliers in the predictions. The lowest MAPE value of 6.775 indicates that the random forest model is also the least affected by the size of the original blanks. The GBM model has evaluation metrics similar to those of random forest, being able to capture a similar proportion of variance (0.971) and slightly higher values of MAE (3.549) and RMSE (6.185). In general, multiple linear regression was the model with the worst performance metrics, capturing slightly more variance than the SVM (respective values of 0.963 and 0.961) and being less affected by the presence of outliers (respective values of 7.036 and 7.265).    

Figure 4 presents the regression plots of all trained models. All models present regression lines with evenly distributed predictions, indicative of an absence of bias. However, it is observed for the multiple linear regression and SVM that with increasing flake mass, the range of predictions becomes more dispersed. None of the predictive variables in the multiple linear regression model presented VIF values above the threshold of 10. Average height of retouch (t) presented the highest VIF value (7.27), followed by the GIUR index (4.59). Log transformed maximum thickness and remaining scraper mass presented respective VIF values of 3.81 and 2.35.   

```{r models-metrics}
data.frame(
  Model = c("Mult. Linear Reg.", "SVM Linear", "Random Forest", "GBM"),
  r2 = c(
    round(MLR_Model.BS$results[[3]], 4),
    round(SVML_Model.BS$results[[3]], 4),
    round(RF_Model.BS$results[[5]], 4),
    round(GBM_Model.BS$results[[6]], 4)),
  MAE = c(
    round(MLR_Model.BS$results[[4]], 3),
    round(SVML_Model.BS$results[[4]], 3),
    round(RF_Model.BS$results[[6]], 3),
    round(GBM_Model.BS$results[[7]], 3)),
  RMSE = c(
    round(MLR_Model.BS$results[[2]], 3),
    round(SVML_Model.BS$results[[2]], 3),
    round(RF_Model.BS$results[[4]], 3),
    round(GBM_Model.BS$results[[5]], 3)),
  MAPE = c(
    round(MLmetrics::MAPE(MLR_Model.BS$pred$pred, MLR_Model.BS$pred$obs)*100, 3),
    round(MLmetrics::MAPE(SVML_Model.BS$pred$pred, SVML_Model.BS$pred$obs)*100, 3),
    round(MLmetrics::MAPE(RF_Model.BS$pred$pred, RF_Model.BS$pred$obs)*100, 3),
    round(MLmetrics::MAPE(GBM_Model.BS$pred$pred, GBM_Model.BS$pred$obs)*100, 3))
)
```


```{r regression-plots-mass}
#### Put all predictions of each model ####

Predicted.MLR <- as.data.frame(MLR_Model.BS$pred) %>% 
  group_by(rowIndex) %>% 
  summarise(
    mean.pred = round(mean(pred),2),
    mean.obs = round(mean(obs),2)) %>% 
  mutate(
    Rema.Weight = Data2$Rem.Weight,
    Real.W.Retrieved = Data2$W.Retrieved) %>% 
  mutate(
    Pred.W.Retrieved = (mean.pred - Rema.Weight),
    Pred.Curated = 100 - (Rema.Weight/mean.pred)*100,
    Real.Curated = 100 - (Rema.Weight/mean.obs )*100,
    Residuals = mean.obs - mean.pred,
    Abs.W.Ret = abs(Pred.W.Retrieved),
    Pred.Curated2 = 100 - (Rema.Weight/mean.pred)*100)


Predicted.SVML <- as.data.frame(SVML_Model.BS$pred) %>% 
  group_by(rowIndex) %>% 
  summarise(
    mean.pred = round(mean(pred),2),
    mean.obs = round(mean(obs),2)) %>% 
  mutate(
    Rema.Weight = Data2$Rem.Weight,
    Real.W.Retrieved = Data2$W.Retrieved) %>% 
  mutate(
    Pred.W.Retrieved = (mean.pred - Rema.Weight),
    Pred.Curated = 100 - (Rema.Weight/mean.pred)*100,
    Real.Curated = 100 - (Rema.Weight/mean.obs )*100,
    Residuals = mean.obs - mean.pred,
    Abs.W.Ret = abs(Pred.W.Retrieved),
    Pred.Curated2 = 100 - (Rema.Weight/mean.pred)*100)


Predicted.RF <- as.data.frame(RF_Model.BS$pred) %>% 
  group_by(rowIndex) %>% 
  summarise(
    mean.pred = round(mean(pred),2),
    mean.obs = round(mean(obs),2)) %>% 
  mutate(
    Rema.Weight = Data2$Rem.Weight,
    Real.W.Retrieved = Data2$W.Retrieved) %>% 
  mutate(
    Pred.W.Retrieved = (mean.pred - Rema.Weight),
    Pred.Curated = 100 - (Rema.Weight/mean.pred)*100,
    Real.Curated = 100 - (Rema.Weight/mean.obs )*100,
    Residuals = mean.obs - mean.pred,
    Abs.W.Ret = abs(Pred.W.Retrieved),
    Pred.Curated2 = 100 - (Rema.Weight/mean.pred)*100)


Predicted.GBM <- as.data.frame(GBM_Model.BS$pred) %>% 
  group_by(rowIndex) %>% 
  summarise(
    mean.pred = round(mean(pred),2),
    mean.obs = round(mean(obs),2)) %>% 
  mutate(
    Rema.Weight = Data2$Rem.Weight,
    Real.W.Retrieved = Data2$W.Retrieved) %>% 
  mutate(
    Pred.W.Retrieved = (mean.pred - Rema.Weight),
    Pred.Curated = 100 - (Rema.Weight/mean.pred)*100,
    Real.Curated = 100 - (Rema.Weight/mean.obs )*100,
    Residuals = mean.obs - mean.pred,
    Abs.W.Ret = abs(Pred.W.Retrieved),
    Pred.Curated2 = 100 - (Rema.Weight/mean.pred)*100)


#### Plots of each regression model #####

ggpubr::ggarrange(
  
  (
    Predicted.MLR %>% 
      ggplot(aes(mean.obs, mean.pred)) +
      geom_point() +
      scale_x_continuous(breaks = seq(0, 220, 20), lim = c(0, 220)) +
      scale_y_continuous(breaks = seq(0, 220, 20), lim = c(0, 220)) +
      ggtitle(label = "Multiple Linear regression model") + 
      geom_smooth(method = "lm") +
      coord_fixed() +
      theme_light() +
      ylab("Predicted mass") +
      xlab("Observed mass") +
      theme(
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 6, color = "black"),
        plot.title = element_text(size = 8))
  ),
  (
    Predicted.SVML %>% 
      ggplot(aes(mean.obs, mean.pred)) +
      geom_point() +
      scale_x_continuous(breaks = seq(0, 220, 20), lim = c(0, 220)) +
      scale_y_continuous(breaks = seq(0, 220, 20), lim = c(0, 220)) +
      ggtitle(label = "SVML model") + 
      geom_smooth(method = "lm") +
      coord_fixed() +
      theme_light() +
      ylab("Predicted mass") +
      xlab("Observed mass") +
      theme(
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 6, color = "black"),
        plot.title = element_text(size = 8))
  ),
  
  (
    Predicted.RF %>% 
      ggplot(aes(mean.obs, mean.pred)) +
      geom_point() +
      scale_x_continuous(breaks = seq(0, 220, 20), lim = c(0, 220)) +
      scale_y_continuous(breaks = seq(0, 220, 20), lim = c(0, 220)) +
      ggtitle(label = "Random Forest model") + 
      geom_smooth(method = "lm") +
      coord_fixed() +
      theme_light() +
      ylab("Predicted mass") +
      xlab("Observed mass") +
      theme(
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 6, color = "black"),
        plot.title = element_text(size = 8))
  ),
  (
    Predicted.GBM %>% 
      ggplot(aes(mean.obs, mean.pred)) +
      geom_point() +
      scale_x_continuous(breaks = seq(0, 220, 20), lim = c(0, 220)) +
      scale_y_continuous(breaks = seq(0, 220, 20), lim = c(0, 220)) +
      ggtitle(label = "GBM model") + 
      geom_smooth(method = "lm") +
      coord_fixed() +
      theme_light() +
      ylab("Predicted mass") +
      xlab("Observed mass") +
      theme(
        axis.title = element_text(size = 7),
        axis.text = element_text(size = 6, color = "black"),
        plot.title = element_text(size = 8))
  ),
  ncol = 2,
  nrow = 2
)
```
\  

Percentage of flake mass consumed by retouch (curation) was calculated using estimations of original flake mass for each model and each episode of resharpening. Table 3 presents performance metrics of the four models when predicting the curation ratio, and Figure 5 presents their corresponding regression plots. Important differences can be observed between models when predicting percentage of mass lost by retouch based on predictions of original mass. Of the four models, only random forest and GBM presented adequate performance metrics. Multiple linear regression presented the lowest performance values, with a linear correlation value of 0.113, while SVM with linear kernel presented a significant higher (but still unsatisfactory) r2 value of 0.399. Both models also presented RMSE values higher than the standard deviation of percentage of mass lost through retouch (SD = 15.92). Visual evaluation of the regression plots indicates that the low performance metrics from both models (multiple linear regression and SVM with linear kernel) seem to be caused by underestimations original flake mass when a small percentage of flake mass has been removed by retouch. As a result, multiple linear regression estimated an original scraper mass value lower than remaining scraper mass in 127 cases. SVM with linear kernel estimated an original scraper mass lower than remaining scraper mass in 118 cases.

```{r regression-plots-curated}
ggpubr::ggarrange(
  (
    Predicted.MLR %>% 
      ggplot(aes(Real.Curated, Pred.Curated)) +
      geom_point(alpha = 0.6) +
      geom_smooth(method = "lm") +
      scale_x_continuous(breaks = seq(-30, 70, 10), lim = c(-30, 70)) +
      scale_y_continuous(breaks = seq(-30, 70, 10), lim = c(-30, 70)) +
      ggtitle(label = "Multiple Linear Regression model") +
      coord_fixed() +
      theme_light() +
      geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.75) +
      geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.75) +
      ylab("Predicted % of mass lost by retouch (curation)") +
      xlab("Observed % of mass lost by retouch (curation)") +
      theme(
        axis.title = element_text(size = 6.5),
        axis.text = element_text(size = 6, color = "black"),
        plot.title = element_text(size = 8))
  ),
  (
    Predicted.SVML %>% 
      ggplot(aes(Real.Curated, Pred.Curated)) +
      geom_point(alpha = 0.6) +
      geom_smooth(method = "lm") +
      scale_x_continuous(breaks = seq(-30, 70, 10), lim = c(-30, 70)) +
      scale_y_continuous(breaks = seq(-30, 70, 10), lim = c(-30, 70)) +
      ggtitle(label = "SVML model") +
      coord_fixed() +
      theme_light() +
      geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.75) +
      geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.75) +
      ylab("Predicted % of mass lost by retouch (curation)") +
      xlab("Observed % of mass lost by retouch (curation)") +
      theme(
        axis.title = element_text(size = 6.5),
        axis.text = element_text(size = 6, color = "black"),
        plot.title = element_text(size = 8))
  ),
  (
    Predicted.RF %>% 
      ggplot(aes(Real.Curated, Pred.Curated)) +
      geom_point(alpha = 0.6) +
      geom_smooth(method = "lm") +
      scale_x_continuous(breaks = seq(-15, 70, 10), lim = c(-15, 70)) +
      scale_y_continuous(breaks = seq(-15, 70, 10), lim = c(-15, 70)) +
      ggtitle(label = "Random Forest Model") +
      coord_fixed() +
      theme_light() +
      geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.75) +
      geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.75) +
      ylab("Predicted % of mass lost by retouch (curation)") +
      xlab("Observed % of mass lost by retouch (curation)") +
      theme(
        axis.title = element_text(size = 6.5),
        axis.text = element_text(size = 6, color = "black"),
        plot.title = element_text(size = 8))
  ),
  (
    Predicted.GBM %>% 
      ggplot(aes(Real.Curated, Pred.Curated)) +
      geom_point(alpha = 0.6) +
      geom_smooth(method = "lm") +
      scale_x_continuous(breaks = seq(-15, 70, 10), lim = c(-15, 70)) +
      scale_y_continuous(breaks = seq(-15, 70, 10), lim = c(-15, 70)) +
      ggtitle(label = "GBM model") + 
      coord_fixed() +
      theme_light() +
      geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.75) +
      geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.75) +
      ylab("Predicted % of mass lost by retouch (curation)") +
      xlab("Observed % of mass lost by retouch (curation)") +
      theme(
        axis.title = element_text(size = 6.5),
        axis.text = element_text(size = 6, color = "black"),
        plot.title = element_text(size = 8))
  ),
  ncol = 2,
  nrow = 2)
```

Linear correlation values indicate that random forest and GBM perform at least twice as well than the SVM with linear kernel, with respective r2 values of 0.839 (random forest), and 0.805 (GBM).  Both models also present RMSE values lower than the SD of the sample, indicative of a good fit of the model. The random forest model presents the best performance metrics, with the highest linear correlation value (r2 = 0.839) and lowest values for MAE (4.662), RMSE (6.485) and MAPE (0.365). Random forest is closely followed by the GBM model, with a linear correlation value of 0.805. Visual evaluation of the regression plots (Figure 5) reinforces the notion of the good performance of random forest and GBM. In both cases prediction points are evenly distributed among the regression lines, and clustering of points in the lower values (due to over or underestimation of curation) are absent. GBM estimated an original scraper mass lower than the remaining scraper mass in 8 cases, while random forest made the same error in only three cases.   

```{r model-metrics-curation}
data.frame(
  Model = c("MLR", "SVML", "Random Forest", "GBM"),
  `r2 W retrieved` = c(
    summary(lm(Real.W.Retrieved ~ Abs.W.Ret, Predicted.MLR))$r.squared %>% round(3),
    summary(lm(Real.W.Retrieved ~ Abs.W.Ret, Predicted.SVML))$r.squared %>% round(3),
    summary(lm(Real.W.Retrieved ~ Abs.W.Ret, Predicted.RF))$r.squared %>% round(3),
    summary(lm(Real.W.Retrieved ~ Abs.W.Ret, Predicted.GBM))$r.squared %>% round(3)
  ),
  `r2 Curated` = c(
    summary(lm(Real.Curated ~ Pred.Curated2, Predicted.MLR))$r.squared %>% round(3),
    summary(lm(Real.Curated ~ Pred.Curated2, Predicted.SVML))$r.squared %>% round(3),
    summary(lm(Real.Curated ~ Pred.Curated2, Predicted.RF))$r.squared %>% round(3),
    summary(lm(Real.Curated ~ Pred.Curated2, Predicted.GBM))$r.squared %>% round(3)),
  MAE = c(
    round(caret::MAE(Predicted.MLR$Real.Curated, Predicted.MLR$Pred.Curated2), 3),
    round(caret::MAE(Predicted.SVML$Real.Curated, Predicted.SVML$Pred.Curated2),3),
    round(caret::MAE(Predicted.RF$Real.Curated, Predicted.RF$Pred.Curated2), 3),
    round(caret::MAE(Predicted.GBM$Real.Curated, Predicted.GBM$Pred.Curated2), 3)),
  RMSE = c(
    round(caret::RMSE(Predicted.MLR$Real.Curated, Predicted.MLR$Pred.Curated2), 3),
    round(caret::RMSE(Predicted.SVML$Real.Curated, Predicted.SVML$Pred.Curated2),3),
    round(caret::RMSE(Predicted.RF$Real.Curated, Predicted.RF$Pred.Curated2), 3),
    round(caret::RMSE(Predicted.GBM$Real.Curated, Predicted.GBM$Pred.Curated2), 3)),
  MAPE = c(
    round(MLmetrics::MAPE(Predicted.MLR$Real.Curated, Predicted.MLR$Pred.Curated2), 3),
    round(MLmetrics::MAPE(Predicted.SVML$Real.Curated, Predicted.SVML$Pred.Curated2), 3),
    round(MLmetrics::MAPE(Predicted.RF$Real.Curated, Predicted.RF$Pred.Curated2), 3),
    round(MLmetrics::MAPE(Predicted.GBM$Real.Curated, Predicted.GBM$Pred.Curated2), 3)))

```
\  

### **Bias, limitations of the best model (random forest) and comparison with other models.**   

The episodes of resharpening generated 130 cases where the flat flake problem is observed and 564 cases where this problem is not observed. Students t-test comparing the residual distribution of flat and non-flat flakes shows no statistical differences (t = -1.93; p = 0.17) for all episodes of resharpening. When selecting flakes that had four or more resharpening episodes, no statistical significance is observed for the residual distribution (t = -0.79; p = 0.43). This indicates that for the given sample and the given predictive variables, random forest predictions are not affected by the flat flake problem.    

Figure 6 presents the residual (when predicting original flake mass) distribution of continuous GIUR values and the same divided into five intervals. A density plot indicates that, on a general level, residuals tend to peak at the zero value. However, with increasing GIUR intervals the peak among the zero value diminishes. Scatter and box plots also indicate that, for higher GIUR values and intervals, a greater range of residual values is present. This indicates that, although residuals are evenly distributed around 0, the accuracy of predictions from the random forest diminishes among heavily retouched flakes (with GIUR values above 0.8).    

Although knowing how many resharpening episodes an archaeological stone has undergone tool is unlikely, the availability of the data in a controlled experimentation allows for a better understanding of possible bias in the model. For the first two episodes of retouch, statistical differences were present between values of actual curation and estimated curation (t = 4.11; p < 0.001 and t = 2.31; p = 0.02). Statistical differences between estimated and actual curation ratios are absent for resharpening episodes three to seven. Statistically significant differences between estimated and real curated values are present for scrapers which underwent eight or more resharpening episodes (t = -3.73; p < 0.001).   


```{r}
Pred.RF <- RF_Model.BS$pred %>%
  group_by(rowIndex) %>%
  summarise(
    pred = mean(pred),
    obs = mean(obs)) %>%
  mutate(
    Residuals = obs - pred)


Pred.RF <- Data2 %>%
  select(Flake.ID, Sec.Trans, Termin, Episode,
         W.Retrieved.g, Rem.Weight.g, Mean.t, Log.M.Thick, GIUR) %>%
  cbind(Pred.RF) %>% mutate(
    Cat.GIUR = cut(GIUR, 5)) %>% 
  mutate(
    Cat.GIUR = case_when(
      Cat.GIUR == "(0.124,0.3]" ~ "0.124 - 0.3",
      Cat.GIUR == "(0.3,0.475]" ~ "0.3 - 0.475",
      Cat.GIUR == "(0.475,0.65]" ~ "0.475 - 0.65",
      Cat.GIUR == "(0.65,0.825]" ~ "0.65 - 0.825",
      Cat.GIUR == "(0.825,1]" ~ "0.825 - 1")
  )

ggpubr::ggarrange(
  (
    Pred.RF %>% ggplot(aes(Residuals, color = Cat.GIUR)) +
      geom_density(size = 1) + 
      theme_light() +
      ggsci::scale_color_d3() +
      labs(color = "GIUR intervals") +
      ylab("Density") +
      theme(
        legend.position = "top",
        axis.text = element_text(size = 8, color = "black")) 
  ),
  
  (
    ggpubr::ggarrange(
      (
        Pred.RF %>% ggplot(aes(GIUR, Residuals)) +
          geom_point() +
          scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +  
          scale_y_continuous(breaks = seq(-50, 50, 10), lim = c(-50, 50)) +
          theme_light() +
          theme(
            axis.text = element_text(color = "black", size = 8)
          )
      ),
      (
        Pred.RF %>% ggplot(aes(Cat.GIUR, Residuals)) +
          geom_boxplot() +
          scale_y_continuous(breaks = seq(-50, 50, 10), lim = c(-50, 50)) + 
          theme_light() +
          xlab("GIUR intervals") +
          theme(
            axis.text = element_text(color = "black", size = 8)
          )
      ), 
      ncol = 2)
  ),
  nrow = 2
  )

```

```{r}
read.csv("Data/RF-results-leaving-flake-sequences.csv")
```

Figure 7 presents real and estimated values of curation according to each resharpening episode. Considering all episodes of retouch, no statistically significant difference is present between values of predicted and actual curation (t = 1.12, df = 1382.7; p = 0.262). However, a statistically significant difference is present between predicted and actual values of curation for the first episode of retouch (t = 3.811, df = 224.83; p < 0.001). This indicates that for scrapers which have undergone very light retouch, the random forest model will slightly overpredict their curation ratio. Statistically significant differences are also present between values of predicted and actual curation on flakes which have undergone eight or more resharpening episodes (t = -3.731, df = 48.493; p < 0.001). This means that scrapers which have undergone multiple episodes of retouch, with more than 50% of their mass removed, will have underpredicted curated ratios (Figure 7).    

Table 4 presents the performance metrics values of the random forest model when leaving out 10% of flakes and all their resharpening episodes. Performance metrics present marginally lower values (Table 2). The Linear correlation (r2) decreases from 0.974 to 0.96, MAE increases from 3.297 to 3.989, RMSE increases from 5.917 to 7.478, and MAPE increases from 6.775 to 7.929.    

When compared to other models using the same sample of flakes (Table 5), the random forest presents much better performance metrics. Neither of the two models presented linear correlation values (r2) above the 0.8 threshold when predicting original flake mass.    





## **References**   

</div> 



